{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dissect_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrederikWarburg/CS-294-131-Trustworthy-Deep-Learning/blob/master/dissect_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "i_sBdOspr-zL",
        "colab_type": "code",
        "outputId": "4ece2eda-8e45-40ef-f47e-3f4a6c873f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+git://github.com/davidbau/quick-netdissect.git#egg=netdissect"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: netdissect from git+git://github.com/davidbau/quick-netdissect.git#egg=netdissect in /usr/local/lib/python3.6/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from netdissect) (1.0.1.post2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from netdissect) (1.14.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from netdissect) (1.1.0)\n",
            "Requirement already satisfied: Pillow>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from netdissect) (4.1.1)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.6/dist-packages (from netdissect) (4.28.1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from netdissect) (0.2.2.post3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=4.1.0->netdissect) (0.46)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->netdissect) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qdRVoamxCyUF",
        "colab_type": "code",
        "outputId": "91393a25-927e-4d20-e873-efef76b44925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qmLgm0-vs6sk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size, latent_dim, dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "        self.feature_sizes = int(self.img_size[0] / 16), int(self.img_size[1] / 16)\n",
        "\n",
        "        self.latent_to_features = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8 * dim * self.feature_sizes[0] * self.feature_sizes[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.features_to_image = nn.Sequential(\n",
        "            nn.ConvTranspose2d(8 * dim, 4 * dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4 * dim),\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ConvTranspose2d(dim, self.img_size[2], 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Map latent into appropriate size for transposed convolutions\n",
        "        x = self.latent_to_features(input_data)\n",
        "        # Reshape\n",
        "        x = x.view(-1, 8 * self.dim, self.feature_sizes[0], self.feature_sizes[1])\n",
        "        # Return generated image\n",
        "        return self.features_to_image(x)\n",
        "\n",
        "    def sample_latent(self, num_samples):\n",
        "        return torch.randn((num_samples, self.latent_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyDPTFay_xOO",
        "colab_type": "code",
        "outputId": "18f45ded-1de2-47e5-8973-7ae1801f6b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "generator = torch.load(\"gen_mnist_model_epoch_200.pt\", map_location='cpu')\n",
        "print(generator)\n",
        "# generater.load_state_dict(torch.load(\".sample_data/gen_mnist_model_epoch_200.pt\", map_location='cpu'))\n",
        "# generater.eval()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (latent_to_features): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (features_to_image): Sequential(\n",
            "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PTWwHQA2tnxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "\n",
        "def get_multi_mnist_dataloaders(batch_size=128):\n",
        "    # Resize images so they are a power of 2\n",
        "    all_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_data = MultiMNIST('MNIST_synthetic.h5',  train = True, transform = all_transforms)\n",
        "    test_data = MultiMNIST('MNIST_synthetic.h5', train = False, transform = all_transforms)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=16)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "class MultiMNIST(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, path = 'MNIST_synthetic.h5', train = True, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        super(MultiMNIST, self).__init__()\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "\n",
        "        f = h5py.File(self.path, 'r')\n",
        "\n",
        "        if train:\n",
        "            self.X = list(f['train_dataset'])\n",
        "            self.y = list(f['train_labels'])\n",
        "            self.seg = list(f['train_segmentations'])\n",
        "\n",
        "        else:\n",
        "            self.X = list(f['test_dataset'])\n",
        "            self.y = list(f['test_labels'])\n",
        "            self.seg = list(f['test_segmentations'])\n",
        "\n",
        "        if False:\n",
        "            self.X = list(f['val_dataset'])\n",
        "            self.y = list(f['val_labels'])\n",
        "            self.seg = list(f['val_segmentations'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return DataLoaderIter(self)\n",
        "\n",
        "    def __getitem__(self, index):#, seg=False):\n",
        "\n",
        "        img = self.X[index]\n",
        "        target = self.y[index]\n",
        "        seg = self.seg[index]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img#, target, seg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxcFUhfCg3pG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "outputId": "bdc8b42d-acf4-4a0b-b125-457244e0f667"
      },
      "cell_type": "code",
      "source": [
        "from netdissect import retain_layers, dissect\n",
        "from netdissect import ReverseNormalize\n",
        "\n",
        "generator.eval()\n",
        "generator.cuda()\n",
        "  \n",
        "#   for name, layer in generator.named_modules():\n",
        "#     print(name)\n",
        "#============ result: \n",
        "# latent_to_features\n",
        "# latent_to_features.0\n",
        "# latent_to_features.1\n",
        "# features_to_image\n",
        "# features_to_image.0\n",
        "# features_to_image.1\n",
        "# features_to_image.2\n",
        "# features_to_image.3\n",
        "# features_to_image.4\n",
        "# features_to_image.5\n",
        "# features_to_image.6\n",
        "# features_to_image.7\n",
        "# features_to_image.8\n",
        "# features_to_image.9\n",
        "# features_to_image.10\n",
        "\n",
        "retain_layers(generator, ['features_to_image.0',\n",
        "                          'features_to_image.1', \n",
        "                          'features_to_image.2', \n",
        "                          'features_to_image.3', \n",
        "                          'features_to_image.4',\n",
        "                          'features_to_image.5',\n",
        "                          'features_to_image.6',\n",
        "                          'features_to_image.7',\n",
        "                          'features_to_image.8',\n",
        "                          'features_to_image.9',\n",
        "                          'features_to_image.10'])\n",
        "bds, _ = get_multi_mnist_dataloaders()\n",
        "dissect('sample_data/', generator, bds,\n",
        "        recover_image=ReverseNormalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        batch_size=100,\n",
        "        examples_per_unit=10)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f43e00f0278>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-02c45c3027f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mrecover_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mReverseNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         examples_per_unit=10)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/netdissect/dissection.py\u001b[0m in \u001b[0;36mdissect\u001b[0;34m(outdir, model, dataset, recover_image, quantile_threshold, iou_threshold, examples_per_unit, batch_size, num_workers, make_images, make_labels, make_report, make_single_images, netname, meta, settings)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 pin_memory=(device.type == 'cuda'))\n\u001b[1;32m     71\u001b[0m         quantiles, topk = collect_quantiles_and_topk(model, segloader,\n\u001b[0;32m---> 72\u001b[0;31m                 recover_image=recover_image, k=examples_per_unit)\n\u001b[0m\u001b[1;32m     73\u001b[0m         levels = {k: qc.quantiles([1.0 - quantile_threshold])[:,0]\n\u001b[1;32m     74\u001b[0m                 for k, qc in quantiles.items()}\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/netdissect/dissection.py\u001b[0m in \u001b[0;36mcollect_quantiles_and_topk\u001b[0;34m(model, segloader, recover_image, k, resolution)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Quantiles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;31m# We don't actually care about the model output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\nTypeError: 'DataLoader' object does not support indexing\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3rpC_C88qsv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVqROMVQqiHc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}