{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dissect_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrederikWarburg/CS-294-131-Trustworthy-Deep-Learning/blob/master/dissect_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "i_sBdOspr-zL",
        "colab_type": "code",
        "outputId": "f1e4681e-8a87-4db7-a15a-3d191636249e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+git://github.com/davidbau/quick-netdissect.git#egg=netdissect"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting netdissect from git+git://github.com/davidbau/quick-netdissect.git#egg=netdissect\n",
            "  Cloning git://github.com/davidbau/quick-netdissect.git to /tmp/pip-install-fq4hn8gl/netdissect\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from netdissect) (1.14.6)\n",
            "Requirement already satisfied: Pillow>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from netdissect) (4.1.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from netdissect) (1.1.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from netdissect) (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from netdissect) (0.2.2.post3)\n",
            "Requirement already satisfied: tqdm>=4.23.4 in /usr/local/lib/python3.6/dist-packages (from netdissect) (4.28.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=4.1.0->netdissect) (0.46)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->netdissect) (1.11.0)\n",
            "Building wheels for collected packages: netdissect\n",
            "  Building wheel for netdissect (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-q5v5u7h6/wheels/95/03/aa/c38949a7269a6ee2f0ceb9e98b54e67fc7f9bfc5b47d1c5e4f\n",
            "Successfully built netdissect\n",
            "Installing collected packages: netdissect\n",
            "Successfully installed netdissect-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qdRVoamxCyUF",
        "colab_type": "code",
        "outputId": "6eafacf9-4ed4-467a-da16-a6c1173b9a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qmLgm0-vs6sk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size, latent_dim, dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "        self.feature_sizes = int(self.img_size[0] / 16), int(self.img_size[1] / 16)\n",
        "\n",
        "        self.latent_to_features = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8 * dim * self.feature_sizes[0] * self.feature_sizes[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.features_to_image = nn.Sequential(\n",
        "            nn.ConvTranspose2d(8 * dim, 4 * dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4 * dim),\n",
        "            nn.ConvTranspose2d(4 * dim, 2 * dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(2 * dim),\n",
        "            nn.ConvTranspose2d(2 * dim, dim, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            nn.ConvTranspose2d(dim, self.img_size[2], 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Map latent into appropriate size for transposed convolutions\n",
        "        x = self.latent_to_features(input_data)\n",
        "        # Reshape\n",
        "        x = x.view(-1, 8 * self.dim, self.feature_sizes[0], self.feature_sizes[1])\n",
        "        # Return generated image\n",
        "        return self.features_to_image(x)\n",
        "\n",
        "    def sample_latent(self, num_samples):\n",
        "        return torch.randn((num_samples, self.latent_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyDPTFay_xOO",
        "colab_type": "code",
        "outputId": "76ced54a-5a14-490e-a2d8-9ec64afd130e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "generator = torch.load(\"gen_mnist_model_epoch_200.pt\", map_location='cpu')\n",
        "print(generator)\n",
        "# generater.load_state_dict(torch.load(\".sample_data/gen_mnist_model_epoch_200.pt\", map_location='cpu'))\n",
        "# generater.eval()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (latent_to_features): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (features_to_image): Sequential(\n",
            "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PTWwHQA2tnxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "\n",
        "def get_multi_mnist_dataloaders(batch_size=128):\n",
        "    # Resize images so they are a power of 2\n",
        "    all_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "#         transforms.Resize(32),\n",
        "        transforms.Resize(100),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_data = MultiMNIST('MNIST_synthetic.h5',  train = True, transform = all_transforms)\n",
        "    test_data = MultiMNIST('MNIST_synthetic.h5', train = False, transform = all_transforms)\n",
        "\n",
        "    # Create dataloaders\n",
        "#     train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=16)\n",
        "#     test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16)\n",
        "#     return train_loader, test_loader\n",
        "    return train_data, test_data\n",
        "\n",
        "class MultiMNIST(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, path = 'MNIST_synthetic.h5', train = True, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        super(MultiMNIST, self).__init__()\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "\n",
        "        f = h5py.File(self.path, 'r')\n",
        "\n",
        "        if train:\n",
        "            self.X = list(f['train_dataset'])\n",
        "            self.y = list(f['train_labels'])\n",
        "            self.seg = list(f['train_segmentations'])\n",
        "\n",
        "        else:\n",
        "            self.X = list(f['test_dataset'])\n",
        "            self.y = list(f['test_labels'])\n",
        "            self.seg = list(f['test_segmentations'])\n",
        "\n",
        "        if False:\n",
        "            self.X = list(f['val_dataset'])\n",
        "            self.y = list(f['val_labels'])\n",
        "            self.seg = list(f['val_segmentations'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "#     def __iter__(self):\n",
        "#         return DataLoaderIter(self)\n",
        "\n",
        "    def __getitem__(self, index):#, seg=False):\n",
        "\n",
        "        img = self.X[index]\n",
        "        target = self.y[index]\n",
        "        seg = self.seg[index]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target, seg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxcFUhfCg3pG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from netdissect import retain_layers, dissect\n",
        "from netdissect import ReverseNormalize\n",
        "\n",
        "generator.eval()\n",
        "generator.cuda()\n",
        "  \n",
        "#   for name, layer in generator.named_modules():\n",
        "#     print(name)\n",
        "#============ result: \n",
        "# latent_to_features\n",
        "# latent_to_features.0\n",
        "# latent_to_features.1\n",
        "# features_to_image\n",
        "# features_to_image.0\n",
        "# features_to_image.1\n",
        "# features_to_image.2\n",
        "# features_to_image.3\n",
        "# features_to_image.4\n",
        "# features_to_image.5\n",
        "# features_to_image.6\n",
        "# features_to_image.7\n",
        "# features_to_image.8\n",
        "# features_to_image.9\n",
        "# features_to_image.10\n",
        "\n",
        "retain_layers(generator, ['features_to_image.0',\n",
        "                          'features_to_image.1', \n",
        "                          'features_to_image.2', \n",
        "                          'features_to_image.3', \n",
        "                          'features_to_image.4',\n",
        "                          'features_to_image.5',\n",
        "                          'features_to_image.6',\n",
        "                          'features_to_image.7',\n",
        "                          'features_to_image.8',\n",
        "                          'features_to_image.9',\n",
        "                          'features_to_image.10'])\n",
        "bds, _ = get_multi_mnist_dataloaders()\n",
        "dissect('sample_data/', generator, bds,\n",
        "        recover_image=ReverseNormalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        batch_size=100,\n",
        "        examples_per_unit=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3rpC_C88qsv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVqROMVQqiHc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}